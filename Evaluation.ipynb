{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b647e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 1.13.1+cu117 with CUDA 1107 (you have 1.12.1+cpu)\n",
      "    Python  3.8.10 (you have 3.8.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import h5py\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "from csv import writer\n",
    "\n",
    "from utils.preprocessing import *\n",
    "from models.model_data import *\n",
    "from models_3d import *\n",
    "from models import controlnet_model_wrapper\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78fa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/nianticlabs/monodepth2/blob/master/evaluate_depth.py\n",
    "\n",
    "def compute_errors(gt, pred):\n",
    "    \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "    \"\"\"\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25     ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "    rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8fb66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import csv\n",
    "\n",
    "interior_design_prompt_1 = \"Intricate, Ornate, Embellished, Elaborate, Detailed, Decorative, Intricately-crafted, Luxurious, Ornamented, and Artistic cloak, open book, sparks, cozy library in background, furniture, fire place, food, wine, pet, chandelier, High Definition, Night time, Photorealism, realistic\"\n",
    "interior_design_prompt_2 = \"Residential home high end futuristic interior, olson kundig, Interior Design by Dorothy Draper, maison de verre, axel vervoordt, award winning photography of an indoor-outdoor living library space, minimalist modern designs, high end indoor/outdoor residential living space, rendered in vray, rendered in octane, rendered in unreal engine, architectural photography, photorealism, featured in dezeen, cristobal palma. 5 chaparral landscape outside, black surfaces/textures for furnishings in outdoor space\"\n",
    "\n",
    "\n",
    "\n",
    "def get_ground_tensors(dataset_path, result_root, condition_type=\"seg\", prompt=interior_design_prompt_1, index_range = (0,0)):\n",
    " \n",
    "    \n",
    "    columns=[#'FID', \n",
    "             'LPIPS', \n",
    "             #'IS', \n",
    "             'CLIPScore', \n",
    "             \"GR-Abs-Rel\", \"GR-Sqr-Rel\", \"GR-RMSE\", \"GR-RMSE-log\", \"GR-thresh-1\", \"GR-thresh-2\", \"GR-thresh-3\", \\\n",
    "             \"Pred-GR-Abs-Rel\", \"Pred-GR-Sqr-Rel\", \"Pred-GR-RMSE\", \"Pred-GR-RMSE-log\",  \\\n",
    "             \"Pred-GR-thresh-1\", \"Pred-GR-thresh-2\", \"Pred-GR-thresh-3\", \\\n",
    "             \"Pred-Gen-Abs-Rel\", \"Pred-Gen-Sqr-Rel\", \"Pred-Gen-RMSE\", \"Pred-Gen-RMSE-log\",  \\\n",
    "             \"Pred-Gen-thresh-1\", \"Pred-Gen-thresh-2\", \"Pred-Gen-thresh-3\"]\n",
    "\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg')\n",
    "    fid = FrechetInceptionDistance(feature=64)\n",
    "    inception = InceptionScore()\n",
    "    clip = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "\n",
    "    f = h5py.File(dataset_path)\n",
    "    rgb_images = f['images']\n",
    "    depth_maps = f['depths']\n",
    "    \n",
    "    \n",
    "    index_begin = index_range[0] if index_range[0] > 0 else 0\n",
    "    index_end = index_range[1] if 0 < index_range[0] <= rgb_images.shape[0] else rgb_images.shape[0]\n",
    "    \n",
    "    #index_limit = min(index_limit,rgb_images.shape[0])\n",
    "    \n",
    "    src_images = []\n",
    "    gen_images = []\n",
    "    ground_depth_images = [] \n",
    "    predict_ground_depth_images = [] \n",
    "    gen_depth_images = [] \n",
    "    \n",
    "    identifier = f\"{condition_type}_range_{index_range[0]}-{index_range[1]}\"\n",
    "    \n",
    "    eval_table_path = result_root + f\"eval_logs/{identifier}.csv\"\n",
    "    \n",
    "    macro_eval_path = result_root + \"eval_logs/macro_eval_metrics.csv\"\n",
    "\n",
    "    for i in range(index_range[0], index_range[1]):\n",
    "        \n",
    "\n",
    "        if not condition_type == \"depth\":\n",
    "            condition_img_path = result_root + f\"depth_maps/{i}_gen_{condition_type}\"\n",
    "            predict_depth_path = result_root + f\"depth_maps/{i}_gen_depth_from_{condition_type}.npy\"\n",
    "            heatmap_path = result_root + f\"depth_map_heat_maps/{i}_depth_heatmap_from_{condition_type}.png\"\n",
    "            gen_pcd_path = result_root + f\"point_clouds/{i}_gen_pcd_from_{condition_type}\"\n",
    "        else:\n",
    "            predict_depth_path = result_root + f\"depth_maps/{i}_gen_depth.npy\"\n",
    "            heatmap_path = result_root + f\"depth_map_heat_maps/{i}_depth_heatmap.png\"\n",
    "            gen_pcd_path = result_root + f\"point_clouds/{i}_gen_pcd\"\n",
    "            \n",
    "        predict_ground_depth_path = result_root + f\"predicted_ground_truth_depth_maps/{i}_predict_ground_depth.npy\"\n",
    "\n",
    "        ground_pcd_path = result_root + f\"point_clouds/{i}_ground_pcd\"\n",
    "        view_setting_path = result_root + \"view_setting.json\"\n",
    "\n",
    "        gen_img_path = result_root + f\"2d_images/{i}_generated_from_{condition_type}.png\"\n",
    "        \n",
    "        predict_ground_depth_map = np.load(predict_ground_depth_path)\n",
    "        predict_depth_map = np.load(predict_depth_path)\n",
    "        image_resolution = predict_depth_map.shape[0]\n",
    "        \n",
    "        src_img_np, ground_depth_map = prepare_nyu_data(rgb_images[i], depth_maps[i], image_resolution=image_resolution)    \n",
    "        \n",
    "        gen_img = Image.open(gen_img_path)\n",
    "        gen_img_np = np.array(gen_img)\n",
    "\n",
    "\n",
    "        predict_ground_depth_map_aligned = align_midas( predict_ground_depth_map, ground_depth_map)\n",
    "        predict_depth_map_aligned = align_midas(predict_depth_map, ground_depth_map)\n",
    "        \n",
    "        # creating image tensors\n",
    "        src_img_tensor = torch.from_numpy(np.moveaxis(src_img_np, 2, 0)).unsqueeze(0)\n",
    "        gen_img_tensor = torch.from_numpy(np.moveaxis(gen_img_np, 2, 0)).unsqueeze(0)\n",
    "        \n",
    "        ground_depth_tensor = torch.from_numpy(ground_depth_map).unsqueeze(0)\n",
    "        predict_ground_depth_tensor = torch.from_numpy(predict_ground_depth_map_aligned).unsqueeze(0)\n",
    "        gen_depth_tensor = torch.from_numpy( predict_depth_map_aligned).unsqueeze(0)\n",
    "        \n",
    "        # -- RGB Image Generation Evaluation\n",
    "        \n",
    "        lpips_score = lpips(src_img_tensor.float()/255, gen_img_tensor.float()/255)\n",
    "        \n",
    "        #fid.update(src_img_tensor, real=True)\n",
    "        #fid.update(gen_img_tensor, real=False)\n",
    "        #fid_score = fid.compute().item()\n",
    "        \n",
    "        #inception.update(gen_img_tensor)\n",
    "        #inception_score = inception.compute().item()\n",
    "        \n",
    "        clip_score = clip(gen_img_tensor, prompt).item()\n",
    "        \n",
    "        \n",
    "        # -- Depth Map Evaluation\n",
    "        \n",
    "        # -- Ground vs Predicted Ground\n",
    "        \n",
    "        g_pg_abs_rel, g_pg_sq_rel, g_pg_rmse, g_pg_rmse_log, g_pg_a1, g_pg_a2, g_pg_a3 = compute_errors(ground_depth_map, predict_ground_depth_map_aligned)\n",
    "        \n",
    "        # -- Predicted Ground vs Predicted Generated\n",
    "        \n",
    "        pg_pgen_abs_rel, pg_pgen_sq_rel, pg_pgen_rmse, pg_pgen_rmse_log, pg_pgen_a1, pg_pgen_a2, pg_pgen_a3 = compute_errors(ground_depth_map, predict_ground_depth_map_aligned)\n",
    "\n",
    "        # -- Ground vs Predicted Generated\n",
    "        \n",
    "        g_pgen_abs_rel, g_pgen_sq_rel, g_pgen_rmse, g_pgen_rmse_log, g_pgen_a1, g_pgen_a2, g_pgen_a3 = compute_errors(ground_depth_map, predict_ground_depth_map_aligned)\n",
    "        \n",
    "        df.loc[i] = [#fid_score, \n",
    "                     lpips_score.item() ,\n",
    "                     #inception_score, \n",
    "                     clip_score, \n",
    "                    g_pg_abs_rel, g_pg_sq_rel, g_pg_rmse, g_pg_rmse_log, g_pg_a1, g_pg_a2, g_pg_a3, \\\n",
    "                    pg_pgen_abs_rel, pg_pgen_sq_rel, pg_pgen_rmse, pg_pgen_rmse_log, pg_pgen_a1, pg_pgen_a2, pg_pgen_a3, \\\n",
    "                    g_pgen_abs_rel, g_pgen_sq_rel, g_pgen_rmse, g_pgen_rmse_log, g_pgen_a1, g_pgen_a2, g_pgen_a3]\n",
    "        \n",
    "        # storing tensors for metric that require multiple samples\n",
    "        src_images.append(src_img_tensor)\n",
    "        gen_images.append(gen_img_tensor)\n",
    "        \n",
    "        #ground_depth_images.append(ground_depth_tensor)\n",
    "        #predict_ground_depth_images.append(predict_ground_depth_tensor)    \n",
    "        #gen_depth_images.append(gen_depth_tensor)\n",
    "        \n",
    "    \n",
    "    src_image_tensor=torch.cat(src_images)\n",
    "    gen_image_tensor=torch.cat(gen_images)\n",
    "    \n",
    "    fid.update(src_image_tensor, real=True)\n",
    "    fid.update(src_image_tensor, real=False)\n",
    "    fid_score = fid.compute()\n",
    "        \n",
    "    inception.update(gen_image_tensor)\n",
    "    inception_score = inception.compute()\n",
    "    \n",
    "    #ground_depth_tensor=torch.cat(depth_images)\n",
    "    #predict_ground_depth_tensor = torch.cat( predict_ground_depth_images)\n",
    "    #gen_depth_tensor=torch.cat(gen_depth_images)\n",
    "    \n",
    "    #return src_image_tensor, gen_image_tensor, ground_depth_tensor, predict_ground_depth_tensor, gen_depth_tensor\n",
    "    \n",
    "    df.to_csv(eval_table_path, index=False)\n",
    "\n",
    "    with open(macro_eval_path, 'a') as csvfile:\n",
    "        fieldnames = ['identifier','FID', 'IS_mean','IS_std']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writerow({'identifier':identifier, 'FID':fid_score.item(), \n",
    "                         'IS_mean': inception_score[0].item(),\n",
    "                         'IS_std': inception_score[1].item()})\n",
    "    \n",
    "    \n",
    "    return df, fid_score, inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8241e36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\User\\anaconda3\\envs\\stable_diff\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12704\\753864328.py:33: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(dataset_path)\n"
     ]
    }
   ],
   "source": [
    "nyu_path = 'C:/Users/User/Documents/Data_Science/ar_stable_diffusion/data/nyu_depth_v2_labeled.mat'\n",
    "eval_df, fid_score, inception_score = get_ground_tensors(dataset_path=nyu_path, result_root=nyu_result_root, index_range = (0,20)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
