{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbadf88c",
   "metadata": {},
   "source": [
    "# ControlNet Based Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import h5py\n",
    "\n",
    "from diffusers import (ControlNetModel, DiffusionPipeline,\n",
    "                       StableDiffusionControlNetPipeline,\n",
    "                       UniPCMultistepScheduler)\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "from csv import writer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4537e",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34decaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nyu_data(rgb_img, depth_map):\n",
    "\n",
    "    # reshape\n",
    "    img_ = np.empty([rgb_img.shape[2], rgb_img.shape[1], 3])\n",
    "    img_[:,:,0] = rgb_img[0,:,:].T\n",
    "    img_[:,:,1] = rgb_img[1,:,:].T\n",
    "    img_[:,:,2] = rgb_img[2,:,:].T\n",
    "    \n",
    "    depth_np = np.asarray(depth_map.T, dtype=np.float32, order=\"C\" )\n",
    "    return img_.astype(np.uint8), depth_np.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_midas(midas_pred, ground_truth):\n",
    "    ground_truth_invert = 1 / (ground_truth + 10e-6) # invert absolute depth with meters\n",
    "    x = midas_pred.copy().flatten()  # Midas Depth\n",
    "    y = ground_truth_invert.copy().flatten()  # Realsense invert Depth\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    s, t = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    midas_aligned_invert = midas_pred * s + t\n",
    "    midas_aligned = 1 /  (midas_aligned_invert + 10e-6)\n",
    "\n",
    "    return midas_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nicer looking plot titles \n",
    "def break_up_string(text, line_limit = 50):\n",
    "\n",
    "    char_count = 0\n",
    "    new_text = \"\"\n",
    "    for word in text.split():\n",
    "        if not new_text:\n",
    "            new_text = word\n",
    "            char_count = len(word)\n",
    "        elif len(word) + char_count < line_limit:\n",
    "            new_text = \" \".join([new_text,word])\n",
    "            char_count += len(word)\n",
    "        else:\n",
    "            new_text = \"\\n\".join([new_text,word])\n",
    "            char_count = len(word)\n",
    "    return new_text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a86bb",
   "metadata": {},
   "source": [
    "## Loading ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a624e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLNET_MODEL_ID = 'lllyasviel/sd-controlnet-depth'\n",
    "#'runwayml/stable-diffusion-v1-5'\n",
    "\n",
    "BASE_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "controlnet = ControlNetModel.from_pretrained(CONTROLNET_MODEL_ID,\n",
    "                                                     torch_dtype=torch.float32)\n",
    "coltrolnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            safety_checker=None,\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float32)\n",
    "coltrolnet_pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
    "            coltrolnet_pipe.scheduler.config)\n",
    "#coltrolnet_pipe.enable_xformers_memory_efficient_attention()\n",
    "coltrolnet_pipe.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553024d",
   "metadata": {},
   "source": [
    "## Inference on ControlNet\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults ------------\n",
    "image_resolution = 512\n",
    "depth_resolution = 512\n",
    "\n",
    "additional_prompt = 'best quality, extremely detailed'\n",
    "negative_prompt   = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
    "num_steps         = 20\n",
    "guidance_scale    = 9\n",
    "seed              = 1825989188"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119daaad",
   "metadata": {},
   "source": [
    "### Prompt Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "interior_design_prompt_1 = \"Intricate, Ornate, Embellished, Elaborate, Detailed, Decorative, Intricately-crafted, Luxurious, Ornamented, and Artistic cloak, open book, sparks, cozy library in background, furniture, fire place, food, wine, pet, chandelier, High Definition, Night time, Photorealism, realistic\"\n",
    "interior_design_prompt_2 = \"Residential home high end futuristic interior, olson kundig, Interior Design by Dorothy Draper, maison de verre, axel vervoordt, award winning photography of an indoor-outdoor living library space, minimalist modern designs, high end indoor/outdoor residential living space, rendered in vray, rendered in octane, rendered in unreal engine, architectural photography, photorealism, featured in dezeen, cristobal palma. 5 chaparral landscape outside, black surfaces/textures for furnishings in outdoor space\"\n",
    "#interior_design_prompt_3 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c4e10",
   "metadata": {},
   "source": [
    "### Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HWC3(x):\n",
    "    #assert x.dtype == np.uint8\n",
    "    if x.ndim == 2:\n",
    "        x = x[:, :, None]\n",
    "    assert x.ndim == 3\n",
    "    H, W, C = x.shape\n",
    "    assert C == 1 or C == 3 or C == 4\n",
    "    if C == 3:\n",
    "        return x\n",
    "    if C == 1:\n",
    "        return np.concatenate([x, x, x], axis=2)\n",
    "    if C == 4:\n",
    "        color = x[:, :, 0:3].astype(np.float32)\n",
    "        alpha = x[:, :, 3:4].astype(np.float32) / 255.0\n",
    "        y = color * alpha + 255.0 * (1.0 - alpha)\n",
    "        y = y.clip(0, 255).astype(np.uint8)\n",
    "        return y\n",
    "    \n",
    "\n",
    "def resize_image(input_image, resolution):\n",
    "    \n",
    "    if len(input_image.shape) == 3:\n",
    "        H, W, C = input_image.shape\n",
    "    else:\n",
    "         H, W = input_image.shape\n",
    "    H = float(H)\n",
    "    W = float(W)\n",
    "    k = float(resolution) / min(H, W)\n",
    "    H *= k\n",
    "    W *= k\n",
    "    H = int(np.round(H / 64.0)) * 64\n",
    "    W = int(np.round(W / 64.0)) * 64\n",
    "    img = cv2.resize(input_image, (W, H), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "def preprocess_for_controlnet(ground_depth_map):\n",
    "    input_image = HWC3(src_img_np)\n",
    "    input_depth = HWC3(ground_depth)\n",
    "    \n",
    "    input_image = resize_image(input_image, image_resolution)\n",
    "    input_depth = resize_image(input_depth, image_resolution)\n",
    "    return PIL.Image.fromarray(control_image), PIL.Image.fromarray(input_depth)\n",
    "   \n",
    "#src_img_np, ground_depth_map \n",
    "#plt.imshow(ground_depth_map, cmap='RdBu')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1cb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nyu_controlnet(x, is_nyu_ground=True, num_samples=1):\n",
    "\n",
    "    if is_nyu_ground:\n",
    "        a = x* 25.5\n",
    "        depth_pt = 1 / (a + 10e-6)\n",
    "    else:\n",
    "        depth_pt = x\n",
    "        \n",
    "    depth_pt -= np.min(depth_pt)\n",
    "    depth_pt /= np.max(depth_pt)\n",
    "    b = (depth_pt * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    c = HWC3(b)\n",
    "\n",
    "    temp_img = resize_image(c, image_resolution)\n",
    "    H, W, C = temp_img.shape\n",
    "\n",
    "    detected_map = cv2.resize(c, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    #detected_map = np.moveaxis(detected_map, -1, 0)\n",
    "\n",
    "    ##control = torch.from_numpy(detected_map.copy()).float() / 255.0\n",
    "    \n",
    "    #result = Image.fromarray(detected_map)\n",
    "\n",
    "    #result = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "    control_image = Image.fromarray(np.uint8(detected_map))\n",
    "    \n",
    "    return control_image, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_controlnet(source_image, prompt, H, W, guidance_scale = 7.5,  depth_image= None, \n",
    "                     num_inference_steps=50, save_name= '', comparison_save_name= '', ): \n",
    "    \n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    prompt = f'{prompt}, {additional_prompt}'\n",
    "    \n",
    "    #source_image = Image.fromarray(source_image)\n",
    "    #depth_tensor = torch.from_numpy(np.expand_dims(depth_image,axis=0))\n",
    "    \n",
    "    #depth_image = Image.fromarray(depth_image)\n",
    "    \n",
    "    \n",
    "    results = coltrolnet_pipe(prompt=prompt, negative_prompt=negative_prompt, \n",
    "                                  image=depth_image,\n",
    "                                  #image=source_image, \n",
    "                                  height = H,\n",
    "                                  width = W,\n",
    "                                  guidance_scale=guidance_scale, \n",
    "                                  num_inference_steps=num_inference_steps,\n",
    "                                  generator=generator)\n",
    "    \n",
    "    title = break_up_string(prompt)\n",
    "    \n",
    "    fontsize=12\n",
    "    if len(title) / 50 > 3:\n",
    "        fontsize=10\n",
    "\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(source_image)\n",
    "    ax[1].imshow(results.images[0])\n",
    "    fig.suptitle(title, fontsize=fontsize,  y=0.9)\n",
    "    for a in ax:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        \n",
    "    fig.tight_layout()\n",
    "        \n",
    "    if save_name:       \n",
    "        results.images[0].save(save_name)\n",
    "    if comparison_save_name:\n",
    "        fig.savefig(comparison_save_name)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return results.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dc30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6f48d5",
   "metadata": {},
   "source": [
    "## Loading Midas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d501ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"MiDaS\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "elif model_type == \"MiDaS\":\n",
    "    transform = midas_transforms.default_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cbbd8",
   "metadata": {},
   "source": [
    "## Midas Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_depth_map(image, save_name= ''):\n",
    "    \n",
    "    input_batch = transform(image).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=image.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "             align_corners=False,\n",
    "        ).squeeze()\n",
    "\n",
    "        output = prediction.cpu().numpy() \n",
    "        \n",
    "        with open(save_name+'.npy', 'wb') as f:\n",
    "            np.save(f, output, allow_pickle=True, fix_imports=True)\n",
    "        \n",
    "        plt.imshow(output)\n",
    "        plt.show()\n",
    "        \n",
    "        if save_name:\n",
    "            im = Image.fromarray(output).convert('RGB')\n",
    "            im.save(save_name+\".png\")\n",
    "        \n",
    "        print(\"infer depth map done\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e3f0c",
   "metadata": {},
   "source": [
    "## Depth Heatmap Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937afcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_heat_map(ground_depth_map, predict_depth_map, img_id=None, save_name=''):\n",
    "    \n",
    "    depth_diff = ground_depth_map - predict_depth_map\n",
    "    \n",
    "    _min, _max = np.amin(ground_depth_map), np.amax(ground_depth_map)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,3, figsize=(16, 6), layout='constrained')\n",
    "    ax[0].imshow(ground_depth_map, vmin = _min, vmax = _max)\n",
    "    ax[1].imshow(predict_depth_map, vmin = _min, vmax = _max)\n",
    "    \n",
    "    heat_min = -4 \n",
    "    heat_max = 4\n",
    "    \n",
    "    diff = ax[2].imshow(depth_diff, cmap='RdBu_r', vmin = heat_min, vmax = heat_max)\n",
    "\n",
    "    cbar = fig.colorbar(diff, ax=ax[2], shrink=0.6)\n",
    "    cbar.set_label('Ground truth - Predicted', rotation=90, labelpad=5)\n",
    "    cbar.ax.set_yticklabels([\"{:.2}\".format(i) + \" m\" for i in cbar.get_ticks()]) # set ticks of your format\n",
    "    \n",
    "    ax[0].set_title('Ground Truth', fontsize=16)\n",
    "    ax[1].set_title('Generated', fontsize=16)\n",
    "    ax[2].set_title('Difference Heat Map', fontsize=16)\n",
    "    \n",
    "    for a in ax:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "    \n",
    "    if img_id is not None:\n",
    "        fig.suptitle(f\"Depth Maps for Image - {img_id}\", fontsize=18,  y=0.95)\n",
    "    else:\n",
    "        fig.suptitle(f\"Depth Maps\", fontsize=18,  y=0.95)\n",
    "    \n",
    "\n",
    "    if save_name:\n",
    "        fig.savefig(save_name)\n",
    "        \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b3857",
   "metadata": {},
   "source": [
    "## Point Cloud and Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_cloud(rgb_image, depth_image, pcd_path, display=False):\n",
    "    \n",
    "    new_depth_image = o3d.geometry.Image(depth_image)\n",
    "\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        o3d.geometry.Image(rgb_image), o3d.geometry.Image(new_depth_image),\n",
    "    depth_scale=10, convert_rgb_to_intensity=False) \n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        rgbd_image,\n",
    "        o3d.camera.PinholeCameraIntrinsic(\n",
    "            o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault))\n",
    "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.0104,max_nn=12))\n",
    "    \n",
    "    # Flip it, otherwise the pointcloud will be upside down\n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "    \n",
    "    #writing point cloud to file\n",
    "    o3d.io.write_point_cloud(pcd_path, pcd, write_ascii=False, compressed=False, print_progress=False)\n",
    "    \n",
    "    if display:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "    return pcd \n",
    "\n",
    "def capture_pcd_with_view_params(pcd, pcd_path, view_setting_path):\n",
    "    \n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    \n",
    "    vis.create_window(visible=False)\n",
    "    vis.add_geometry(pcd)\n",
    "    \n",
    "    with open(view_setting_path, \"r\") as f: \n",
    "        js = json.load(f)\n",
    "    \n",
    "    vc=vis.get_view_control()\n",
    "    vc.change_field_of_view(js['trajectory'][0]['field_of_view'])\n",
    "    vc.set_front(js['trajectory'][0]['front'])\n",
    "    vc.set_lookat(js['trajectory'][0]['lookat'])\n",
    "    vc.set_up(js['trajectory'][0]['up'])\n",
    "    vc.set_zoom(js['trajectory'][0]['zoom'])\n",
    "    \n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    vis.capture_screen_image(pcd_path)\n",
    "    vis.destroy_window()\n",
    "\n",
    "def get_mesh_from_pcd(pcd, method=\"ball_rolling\"):\n",
    "\n",
    "    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "        \n",
    "        if method == \"poisson\":    \n",
    "            mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd,\n",
    "                depth=10, width=0, scale=1.1, linear_fit=True)\n",
    "            print(mesh)\n",
    "            o3d.visualization.draw_geometries([mesh])\n",
    "            \n",
    "            return mesh, densities\n",
    "        \n",
    "        if method == \"ball_pivoting\":\n",
    "            radii = [0.005, 0.01, 0.02, 0.04]\n",
    "            mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(\n",
    "                pcd, o3d.utility.DoubleVector(radii))\n",
    "            o3d.visualization.draw_geometries([pcd, rec_mesh])\n",
    "            \n",
    "            return mesh\n",
    "        \n",
    "        if method == \"alpha\":\n",
    "            tetra_mesh, pt_map = o3d.geometry.TetraMesh.create_from_point_cloud(pcd)\n",
    "            \n",
    "            for alpha in np.logspace(np.log10(0.5), np.log10(0.1), num=4):\n",
    "                print(f\"alpha={alpha:.3f}\")\n",
    "                mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(\n",
    "                    pcd, alpha, tetra_mesh, pt_map)\n",
    "                mesh.compute_vertex_normals()\n",
    "                o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "                \n",
    "        if method == \"ball_rolling\":\n",
    "            distances = pcd.compute_nearest_neighbor_distance()\n",
    "            avg_dist = np.mean(distances)\n",
    "            radius = 3 * avg_dist\n",
    "            o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(\n",
    "                pcd, o3d.utility.DoubleVector([radius, radius * 2]))\n",
    "            \n",
    "            dec_mesh = mesh.simplify_quadric_decimation(100000)\n",
    "            dec_mesh.remove_degenerate_triangles()\n",
    "            dec_mesh.remove_duplicated_triangles()\n",
    "            dec_mesh.remove_duplicated_vertices()\n",
    "            dec_mesh.remove_non_manifold_edges()\n",
    "            \n",
    "            o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, H, W = prepare_nyu_controlnet(ground_depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9be30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e730c983",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165064e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_path = 'C:/Users/User/Documents/Data_Science/ar_stable_diffusion/data/nyu_depth_v2_labeled.mat'\n",
    "nyu_result_root = './results/NYU/'\n",
    "eval_results_path = nyu_result_root + \"eval_logs.csv\"\n",
    "\n",
    "save_folder = nyu_result_root\n",
    "\n",
    "guidance_scale = 7.5\n",
    "strength=0.5\n",
    "num_inference_steps=30\n",
    "\n",
    "prompt= \"baroque style palace room with ornate marble decorations and statues, landscape paintings on the walls, warm ambient lighting and painted ceiling\"\n",
    "\n",
    "prompt=interior_design_prompt_1\n",
    "\n",
    "# read mat file\n",
    "f = h5py.File(nyu_path)\n",
    "\n",
    "rgb_images = f['images']\n",
    "depth_maps = f['depths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for i in range(0,rgb_images.shape[0]):\n",
    "\n",
    "for i in range(16,rgb_images.shape[0]):\n",
    "    img_id = i\n",
    "    \n",
    "    src_img_np, ground_depth_map = prepare_nyu_data(rgb_images[i], depth_maps[i])\n",
    "    ground_depth, H, W = prepare_nyu_controlnet(ground_depth_map)\n",
    "\n",
    "    gen_img_save_name = save_folder + f\"2d_images/{img_id}_generated.png\"\n",
    "    comparison_save_name = save_folder + f\"2d_images/{img_id}_comparison.png\"\n",
    "        \n",
    "        \n",
    "    gen_img = infer_controlnet(source_image=src_img_np, prompt=prompt, depth_image=ground_depth,\n",
    "                         H=H, W=W, \n",
    "                         guidance_scale = guidance_scale, \n",
    "                         num_inference_steps=num_inference_steps,\n",
    "                         save_name= gen_img_save_name, \n",
    "                         comparison_save_name= comparison_save_name)\n",
    "    \n",
    "    print( type(gen_img))\n",
    "\n",
    "    gen_img_np = np.array(gen_img)\n",
    "    \n",
    "    print(type(gen_img_np))\n",
    "\n",
    "    #ground_depth_map = infer_depth_map(source_img_np)\n",
    "    predict_depth_path = nyu_result_root + f\"depth_maps/{i}_gen_depth\"\n",
    "    predict_depth_map = infer_depth_map(gen_img_np, save_name=predict_depth_path)\n",
    "    \n",
    "    print(type(predict_depth_map))\n",
    "    \n",
    "    groundmap_for_heatmap = resize_image(ground_depth_map, image_resolution)\n",
    "    \n",
    "    predict_depth_map_aligned = align_midas(predict_depth_map, groundmap_for_heatmap)\n",
    "    \n",
    "    print(type(predict_depth_map_aligned))\n",
    "    \n",
    "    heatmap_path = nyu_result_root + f\"depth_map_heat_maps/{i}_depth_heatmap.png\"\n",
    "    heatmap = get_depth_heat_map(groundmap_for_heatmap, predict_depth_map_aligned,\n",
    "                                img_id=i, save_name=heatmap_path)\n",
    "    \n",
    "    ground_pcd_path = nyu_result_root + f\"point_clouds/{i}_ground_pcd\"\n",
    "    gen_pcd_path = nyu_result_root + f\"point_clouds/{i}_gen_pcd\"\n",
    "    \n",
    "    original_pcd = get_point_cloud(src_img_np, ground_depth_map,  pcd_path=ground_pcd_path+\".pcd\")\n",
    "    generated_pcd = get_point_cloud(gen_img_np, predict_depth_map_aligned, pcd_path=gen_pcd_path+\".pcd\")\n",
    "    \n",
    "    view_setting_path = nyu_result_root +\"view_setting.json\"\n",
    "    \n",
    "    capture_pcd_with_view_params(pcd=original_pcd, pcd_path=ground_pcd_path+\".png\", view_setting_path=view_setting_path)\n",
    "    capture_pcd_with_view_params(pcd=generated_pcd, pcd_path=gen_pcd_path+\".png\", view_setting_path=view_setting_path)\n",
    "    \n",
    "    \n",
    "    #eval_list = get_eval_results(src_img_np, gen_img_np, ground_depth_map, predict_depth_map_aligned)\n",
    "    \n",
    "    \n",
    "    #with open(eval_results_path, 'a') as f_object:\n",
    " \n",
    "     #   writer_object = writer(f_object)\n",
    " \n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "      #  writer_object.writerow(eval_list)\n",
    " \n",
    "        # Close the file object\n",
    "       # f_object.close()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273c530",
   "metadata": {},
   "source": [
    "# Rebuilding Point Clouds for Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_point_clouds(i, display=True):\n",
    "\n",
    "\n",
    "    predict_depth_path = nyu_result_root + f\"depth_maps/{i}_gen_depth.npy\"\n",
    "    predict_depth_map = np.load(predict_depth_path)\n",
    "    src_img_np, ground_depth_map = prepare_nyu_data(rgb_images[i], depth_maps[i])\n",
    "    ground_depth, H, W = prepare_nyu_controlnet(ground_depth_map)\n",
    "    gen_img = Image.open(save_folder + f\"2d_images/{i}_generated.png\") \n",
    "    gen_img.show()\n",
    "    gen_img_np = np.array(gen_img)\n",
    "    groundmap_for_heatmap = resize_image(ground_depth_map, image_resolution)\n",
    "    predict_depth_map_aligned = align_midas(predict_depth_map, groundmap_for_heatmap)\n",
    "    \n",
    "    ground_pcd_path = nyu_result_root + f\"point_clouds/{i}_ground_pcd\"\n",
    "    gen_pcd_path = nyu_result_root + f\"point_clouds/{i}_gen_pcd\"\n",
    "\n",
    "    original_pcd = get_point_cloud(src_img_np, ground_depth_map,  pcd_path=ground_pcd_path+\".pcd\", display=display)\n",
    "    generated_pcd = get_point_cloud(gen_img_np, predict_depth_map_aligned, pcd_path=gen_pcd_path+\".pcd\", display=display)\n",
    "    \n",
    "    #pcd = o3d.io.read_point_cloud( nyu_result_root + f\"point_clouds/{i}_ground_pcd.pcd\")\n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return original_pcd, generated_pcd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_point_clouds_ground_depth(i, display=True):\n",
    "\n",
    "\n",
    "    predict_depth_path = nyu_result_root + f\"depth_maps/{i}_gen_depth.npy\"\n",
    "    predict_depth_map = np.load(predict_depth_path)\n",
    "    src_img_np, ground_depth_map = prepare_nyu_data(rgb_images[i], depth_maps[i])\n",
    "    ground_depth, H, W = prepare_nyu_controlnet(ground_depth_map)\n",
    "    gen_img = Image.open(save_folder + f\"2d_images/{i}_generated.png\") \n",
    "    gen_img.show()\n",
    "    gen_img_np = np.array(gen_img)\n",
    "    groundmap_for_heatmap = resize_image(ground_depth_map, image_resolution)\n",
    "    predict_depth_map_aligned = align_midas(predict_depth_map, groundmap_for_heatmap)\n",
    "    \n",
    "    ground_pcd_path = nyu_result_root + f\"point_clouds/{i}_ground_pcd\"\n",
    "    gen_pcd_path = nyu_result_root + f\"point_clouds/{i}_gen_pcd\"\n",
    "\n",
    "    original_pcd = get_point_cloud(src_img_np, ground_depth_map,  pcd_path=ground_pcd_path+\".pcd\", display=display)\n",
    "    generated_pcd = get_point_cloud(gen_img_np, groundmap_for_heatmap, pcd_path=gen_pcd_path+\"_ground_depth.pcd\", display=display)\n",
    "    \n",
    "    #pcd = o3d.io.read_point_cloud( nyu_result_root + f\"point_clouds/{i}_ground_pcd.pcd\")\n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return original_pcd, generated_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def rebuild_point_clouds_heatmap(i, display=True):\n",
    "\n",
    "\n",
    "    predict_depth_path = nyu_result_root + f\"depth_maps/{i}_gen_depth.npy\"\n",
    "    predict_depth_map = np.load(predict_depth_path)\n",
    "    src_img_np, ground_depth_map = prepare_nyu_data(rgb_images[i], depth_maps[i])\n",
    "    ground_depth, H, W = prepare_nyu_controlnet(ground_depth_map)\n",
    "    gen_img = Image.open(save_folder + f\"2d_images/{i}_generated.png\") \n",
    "    gen_img.show()\n",
    "    gen_img_np = np.array(gen_img)\n",
    "    groundmap_for_heatmap = resize_image(ground_depth_map, image_resolution)\n",
    "\n",
    "    predict_depth_map_aligned = align_midas(predict_depth_map, groundmap_for_heatmap)\n",
    "    \n",
    "    \n",
    "    difference_map = groundmap_for_heatmap - predict_depth_map_aligned\n",
    "    \n",
    "    norm = mpl.colors.Normalize(vmin=-4, vmax=4)\n",
    "    cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    heatmap = (m.to_rgba(difference_map)[:,:,:3] * 256).astype(np.uint8)\n",
    "    \n",
    "    gen_pcd_path = nyu_result_root +  f\"point_clouds/{i}_heatmap_pcd.pcd\"\n",
    "\n",
    "    pcd = get_point_cloud(heatmap, predict_depth_map_aligned, pcd_path=gen_pcd_path, display=display)\n",
    "    \n",
    "    #pcd = o3d.io.read_point_cloud( nyu_result_root + f\"point_clouds/{i}_heatmap_pcd.pcd\")\n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_point_clouds(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_point_clouds_ground_depth(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_point_clouds_heatmap(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40106b8f",
   "metadata": {},
   "source": [
    "# Creating Meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606163ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pcd, generated_pcd = rebuild_point_clouds(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mesh_from_pcd(original_pcd, method=\"ball_rolling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e742bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
